# 🕷️ crawling.py 코드 분석 보고서

이 문서는 공공데이터포털(`data.go.kr`)에서 데이터셋 메타데이터와 파일을 수집하는 `crawling.py` 스크립트의 구조와 작동 원리를 분석한 결과입니다.

---

## 📌 개요
- **목적**: 공공데이터포털의 검색 결과 페이지를 순회하며 각 데이터셋의 상세 정보(schema.org JSON)를 수집하고 저장함.
- **주요 기술**: Python, Selenium (Chrome WebDriver), JSON, Pandas.

---

## 🏗️ 핵심 코드 구조 분석

### 1. 환경 설정 및 초기화
- `DOWNLOAD_DIR`: 크롤링된 파일이 저장될 경로를 `downloaded_data_files`로 지정.
- `setup_chrome_options()`: 
    - Chrome의 다운로드 경로를 자동 지정하고, 다운로드 확인 창을 띄우지 않도록 설정.
    - `headless` 옵션(주석 처리됨)을 통해 백그라운드 실행 가능.

### 2. 위치 식별자 (LOCATORS)
- 웹 페이지의 버튼, 링크, 목록 등을 찾기 위한 CSS Selector와 XPATH를 사전 정의하여 코드의 가독성과 유지보수성을 높임.
- 특히 `메타데이터_JSON_링크`를 통해 `schema.org` 형식의 풍부한 메타데이터를 조준함.

### 3. 주요 함수 로직

#### 🟢 `crawl_data_portal()` (메인 컨트롤러)
1. 공공데이터포털 메인 페이지 접속.
2. 초기 검색 버튼을 클릭하여 전체 데이터 목록으로 진입.
3. `DATA_TYPES`에 정의된 4가지 유형(파일데이터, 오픈API 등)을 순차적으로 클릭하며 탐색.

#### 🟡 `crawl_page_items()` (목록 처리기)
- 현재 페이지에 노출된 10개의 아이템을 하나씩 클릭하여 상세 페이지로 이동.
- 각 아이템 처리가 끝나면 `driver.back()`을 통해 목록으로 돌아와 다음 아이템 진행.
- 수집된 모든 정보는 `data/all_crawled_data_summary.json`에 저장.

#### 🔵 `process_detail_page()` (상세 정보 추출기)
- **가장 핵심적인 로직**이 포함됨:
    1. 상세 페이지에서 '다운로드' 버튼(초기 액션 버튼) 클릭.
    2. 드롭다운 메뉴에서 `schema.org` JSON 링크를 찾아 클릭.
    3. **새 탭**으로 열리는 JSON 텍스트를 파싱하여 데이터를 `summary` 딕셔너리에 저장.
    4. 작업 완료 후 새 탭을 닫고 원래 상세 페이지 탭으로 복귀.

---

## 💡 코드의 특징 및 장점
1. **안정성 강화**: `WebDriverWait`와 `EC(expected_conditions)`를 적극 활용하여 페이지 로딩 대기 시간을 유연하게 관리.
2. **오류 복구**: 상세 페이지 처리 중 오류가 발생해도 `try-except`와 탭 관리 로직(`switch_to.window`)을 통해 크롤러가 멈추지 않고 다음 아이템으로 넘어가도록 설계됨.
3. **데이터 표준화**: 공공데이터포털에서 제공하는 `schema.org` 규격의 JSON을 직접 파싱하므로, 매우 정교하고 표준화된 메타데이터 수집이 가능함.

---

## ⚠️ 개선 및 주의 사항
- **페이지네이션**: 현재 코드에서는 1페이지만 크롤링하도록 설정되어 있으며, 전체 데이터 수집을 위해서는 페이지네이션 순회 로직 완성 필요.
- **할당량 및 차단**: 짧은 시간 내에 과도한 요청을 보낼 경우 사이트로부터 IP 차단을 당할 수 있으므로 `time.sleep()`의 적절한 조절이 필요함.
