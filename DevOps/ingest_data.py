import os
import pandas as pd
import time
from dotenv import load_dotenv
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS

# .env ë¡œë“œ
load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env"))

INDEX_PATH = "faiss_index"
CSV_PATH = r"c:\Users\Jang_home\Desktop\git tool\DatasetExplorerAI\DevOps\etl\data\ê³µê³µë°ì´í„°í™œìš©ì§€ì›ì„¼í„°_ê³µê³µë°ì´í„°í¬í„¸ ëª©ë¡ê°œë°©í˜„í™©_20251130.csv"

def ingest():
    print("ğŸš€ FAISS ì¸ë±ìŠ¤ ì‚¬ì „ ìƒì„± ì‹œì‘...")
    
    # 1. ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(CSV_PATH, encoding='utf-8', low_memory=False)
    df['search_text'] = df.apply(lambda x: 
        f"ë°ì´í„°ì…‹ëª…: {str(x['ëª©ë¡ëª…'])}, ì„¤ëª…: {str(x['ì„¤ëª…'])}, í‚¤ì›Œë“œ: {str(x['í‚¤ì›Œë“œ'])}, ì œê³µê¸°ê´€: {str(x['ì œê³µê¸°ê´€'])}", 
        axis=1
    ).fillna("ì •ë³´ ì—†ìŒ")
    
    # 2. ì„ë² ë”© ì„¤ì •
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001", 
        google_api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    # 3. ì¸ë±ì‹± (í• ë‹¹ëŸ‰ ì´ìŠˆë¥¼ ê³ ë ¤í•˜ì—¬ 100ê°œë§Œ ë¨¼ì € ì‹œë„)
    # íŒ: ìœ ë£Œ ê³„ì • í• ë‹¹ëŸ‰ì´ ì¶©ë¶„í•˜ë‹¤ë©´ [:2000] ë“±ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”.
    sample_size = 100 
    texts = df['search_text'].tolist()[:sample_size]
    
    print(f"ğŸ“¦ {len(texts)}ê°œì˜ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤. (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...)")
    
    try:
        vector_db = FAISS.from_texts(texts, embedding=embeddings)
        vector_db.save_local(INDEX_PATH)
        print(f"âœ¨ ì„±ê³µ! '{INDEX_PATH}' í´ë”ì— ì¸ë±ìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ì´ì œ agent.pyë¥¼ ì‹¤í–‰í•˜ë©´ ì´ íŒŒì¼ì„ ì¦‰ì‹œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ êµ¬ê¸€ API í• ë‹¹ëŸ‰ì´ ë¶€ì¡±í•œ ìƒíƒœì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    ingest()
