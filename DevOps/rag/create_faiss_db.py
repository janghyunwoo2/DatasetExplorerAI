"""
CSV ë°ì´í„°ë¥¼ FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬ ë²„ì „)
- 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ë° ì‹œê°„ íš¨ìœ¨ì„± í–¥ìƒ
- ê° ë°°ì¹˜ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
- ëª¨ë“  ë°°ì¹˜ë¥¼ í•˜ë‚˜ì˜ FAISS DBë¡œ ë³‘í•©
"""

import os
import csv
import boto3
import shutil
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_aws import BedrockEmbeddings
from langchain_core.documents import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv('DevOps/rag/teacher/.env')

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
BATCH_SIZE = 100

print("=" * 60)
print("FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹)")
print("=" * 60)

# 1. Bedrock ì„ë² ë”© ê°ì²´ ìƒì„±
print("\n[1/5] Bedrock ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
embeddings = BedrockEmbeddings(
    client=boto3.client(
        service_name="bedrock-runtime",
        region_name=os.getenv("AWS_REGION")
    ),
    model_id="amazon.titan-embed-text-v1"
)
print("âœ… ì´ˆê¸°í™” ì™„ë£Œ\n")

# 2. CSV íŒŒì¼ ì½ê¸°
csv_file_path = "DevOps/rag/data/split_data_01.csv"
print(f"[2/5] CSV íŒŒì¼ ì½ëŠ” ì¤‘: {csv_file_path}")

documents = []
try:
    with open(csv_file_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        
        for idx, row in enumerate(reader):
            description = row.get('ì„¤ëª…', '')
            if not description.strip():
                description = f"{row.get('ëª©ë¡ëª…', '')} {row.get('í‚¤ì›Œë“œ', '')}"
            
            metadata = {
                "ëª©ë¡í‚¤": row.get('ëª©ë¡í‚¤', ''),
                "ëª©ë¡ìœ í˜•": row.get('ëª©ë¡ìœ í˜•', ''),
                "ëª©ë¡ëª…": row.get('ëª©ë¡ëª…', ''),
                "ì œê³µê¸°ê´€": row.get('ì œê³µê¸°ê´€', ''),
                "ì œê³µê¸°ê´€ì½”ë“œ": row.get('ì œê³µê¸°ê´€ì½”ë“œ', ''),
                "ë¶„ë¥˜ì²´ê³„": row.get('ë¶„ë¥˜ì²´ê³„', ''),
                "í‚¤ì›Œë“œ": row.get('í‚¤ì›Œë“œ', ''),
                "ì—…ë°ì´íŠ¸ì£¼ê¸°": row.get('ì—…ë°ì´íŠ¸ ì£¼ê¸°', ''),
                "ì œê³µí˜•íƒœ": row.get('ì œê³µí˜•íƒœ', ''),
                "ë“±ë¡ì¼": row.get('ë“±ë¡ì¼', ''),
                "ìˆ˜ì •ì¼": row.get('ìˆ˜ì •ì¼', ''),
                "URL": row.get('ëª©ë¡ URL', ''),
                "í™•ì¥ì": row.get('í™•ì¥ì(ë°ì´í„°í¬ë§·)', ''),
                "source": csv_file_path,
                "row_index": idx
            }
            
            doc = Document(page_content=description, metadata=metadata)
            documents.append(doc)
    
    print(f"âœ… ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ìƒì„± ì™„ë£Œ\n")
    
except Exception as e:
    print(f"âŒ CSV ì½ê¸° ì˜¤ë¥˜: {e}")
    exit(1)

# 3. ë°°ì¹˜ ì²˜ë¦¬ë¡œ FAISS DB ìƒì„±
print(f"[3/5] ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}ê°œ)")
total_batches = (len(documents) + BATCH_SIZE - 1) // BATCH_SIZE
print(f"ì´ {total_batches}ê°œì˜ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤\n")

batch_db_paths = []
temp_dir = "DevOps/rag/data/temp_batches"
os.makedirs(temp_dir, exist_ok=True)

try:
    for batch_idx in range(total_batches):
        start_idx = batch_idx * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, len(documents))
        batch_docs = documents[start_idx:end_idx]
        
        print(f"ë°°ì¹˜ {batch_idx + 1}/{total_batches}: Document {start_idx+1}~{end_idx} ì²˜ë¦¬ ì¤‘...")
        
        # ë°°ì¹˜ë³„ FAISS DB ìƒì„±
        batch_db = FAISS.from_documents(
            documents=batch_docs,
            embedding=embeddings
        )
        
        # ë°°ì¹˜ ì €ì¥
        batch_path = f"{temp_dir}/batch_{batch_idx}"
        batch_db.save_local(batch_path)
        batch_db_paths.append(batch_path)
        
        print(f"âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ ë° ì €ì¥: {batch_path}")
        print(f"   ì§„í–‰ë¥ : {end_idx}/{len(documents)} ({end_idx*100//len(documents)}%)\n")
    
    print("âœ… ëª¨ë“  ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\n")
    
except Exception as e:
    print(f"âŒ ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# 4. ëª¨ë“  ë°°ì¹˜ ë³‘í•©
print(f"[4/5] FAISS ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•© ì¤‘...")
print(f"{len(batch_db_paths)}ê°œì˜ ë°°ì¹˜ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©í•©ë‹ˆë‹¤...\n")

try:
    # ì²« ë²ˆì§¸ ë°°ì¹˜ ë¡œë“œ
    print("ì²« ë²ˆì§¸ ë°°ì¹˜ ë¡œë“œ ì¤‘...")
    final_db = FAISS.load_local(
        batch_db_paths[0],
        embeddings,
        allow_dangerous_deserialization=True
    )
    print(f"âœ… ë°°ì¹˜ 1 ë¡œë“œ ì™„ë£Œ")
    
    # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ë³‘í•©
    for i, batch_path in enumerate(batch_db_paths[1:], start=2):
        print(f"ë°°ì¹˜ {i} ë³‘í•© ì¤‘...")
        batch_db = FAISS.load_local(
            batch_path,
            embeddings,
            allow_dangerous_deserialization=True
        )
        final_db.merge_from(batch_db)
        print(f"âœ… ë°°ì¹˜ {i} ë³‘í•© ì™„ë£Œ")
    
    print("\nâœ… ëª¨ë“  ë°°ì¹˜ ë³‘í•© ì™„ë£Œ!\n")
    
    # ìµœì¢… DB ì €ì¥
    final_save_path = "DevOps/rag/data/faiss_dataset_db"
    final_db.save_local(final_save_path)
    print(f"âœ… ìµœì¢… FAISS DB ì €ì¥ ì™„ë£Œ: {final_save_path}\n")
    
    # ì„ì‹œ ë°°ì¹˜ íŒŒì¼ ì‚­ì œ
    print("ì„ì‹œ ë°°ì¹˜ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    shutil.rmtree(temp_dir)
    print("âœ… ì •ë¦¬ ì™„ë£Œ\n")
    
except Exception as e:
    print(f"âŒ ë³‘í•© ì˜¤ë¥˜: {e}")
    import traceback
    traceback.print_exc()
    exit(1)

# 5. í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
print("[5/5] í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰")
print("-" * 60)

test_query = "ì˜ë£Œ ë°ì´í„°"
test_results = final_db.similarity_search(test_query, k=5)

print(f"ê²€ìƒ‰ì–´: '{test_query}'")
print(f"ê²€ìƒ‰ ê²°ê³¼: {len(test_results)}ê±´\n")

for i, doc in enumerate(test_results, 1):
    print(f"{i}. ğŸ“Š {doc.metadata.get('ëª©ë¡ëª…', 'N/A')}")
    print(f"   ğŸ¢ ì œê³µê¸°ê´€: {doc.metadata.get('ì œê³µê¸°ê´€', 'N/A')}")
    print(f"   ğŸ“ ë¶„ë¥˜: {doc.metadata.get('ë¶„ë¥˜ì²´ê³„', 'N/A')}")
    print(f"   ğŸ”— URL: {doc.metadata.get('URL', 'N/A')}")
    print()

print("=" * 60)
print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
print(f"ìµœì¢… ì €ì¥ ìœ„ì¹˜: {final_save_path}")
print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
print("=" * 60)
