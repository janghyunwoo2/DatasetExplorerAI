'''
MCP ê¸°ë°˜ RAG ì—ì´ì „íŠ¸ ëª¨ë“ˆ
LangChain MCP Adapterë¥¼ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
'''
from dotenv import load_dotenv
import os
import asyncio
from langchain_core.messages import HumanMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate, MessagesPlaceholder
from langchain_aws import ChatBedrockConverse
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Optional
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

load_dotenv()

# 1. LLM ë“± ê¸°ë³¸ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
llm = ChatBedrockConverse(
    model       = os.getenv("BEDROCK_MODEL_ID"),
    region_name = os.getenv("AWS_REGION"),
    temperature = 0.1,
    max_tokens  = 1000
)

# [MCP ì—°ê²° ì„¤ì •]
# MCP ì„œë²„ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
current_dir = os.path.dirname(os.path.abspath(__file__))
mcp_server_script = os.path.join(current_dir, "mcp_server", "run_mcp_server.py") # [ìˆ˜ì •] íŒŒì¼ëª… ë³€ê²½

import sys

mcp_server_params = StdioServerParameters( # [ìˆ˜ì •] ë³€ìˆ˜ëª… ë³€ê²½ (server_params -> mcp_server_params)
    command=sys.executable, # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ íŒŒì´ì¬ ì¸í„°í”„ë¦¬í„° ì‚¬ìš©
    args=[mcp_server_script],
    env=os.environ.copy()
)

async def call_mcp_tool(tool_name: str, arguments: dict):
    """
    MCP ì„œë²„ì— ì—°ê²°í•˜ì—¬ ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
    ë§¤ í˜¸ì¶œë§ˆë‹¤ ì—°ê²°ì„ ë§ºê³  ëŠìŠµë‹ˆë‹¤ (Stateless).
    """
    async with stdio_client(mcp_server_params) as (read, write): # [ìˆ˜ì •] ë³€ìˆ˜ëª… ë°˜ì˜
        async with ClientSession(read, write) as session:
            # ë„êµ¬ ì´ˆê¸°í™”
            await session.initialize()
            
            # ë„êµ¬ ì‹¤í–‰ ìš”ì²­
            result = await session.call_tool(tool_name, arguments)
            
            # ê²°ê³¼ ë°˜í™˜ (ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì½˜í…ì¸ )
            if result.content and len(result.content) > 0:
                return result.content[0].text
            return "ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."

# ... (í”„ë¡¬í”„íŠ¸ ì„¤ì • ì„¹ì…˜ì€ ê¸°ì¡´ agent_with_garph.pyì™€ ë™ì¼í•˜ê²Œ ìœ ì§€) ...
# 3. í“¨ìƒ· í”„ë¡¬í”„íŠ¸
examples = [
    {
        "input": "í™˜ê²½ ë°ì´í„° ì¶”ì²œí•´ì¤˜",
        "output": """1. **í•´ì–‘í™˜ê²½ê³µë‹¨_í•´ì–‘í™˜ê²½ ì •ë³´**
   - ì œê³µê¸°ê´€: í•´ì–‘í™˜ê²½ê³µë‹¨
   - ë¶„ë¥˜: í™˜ê²½ê¸°ìƒ - í•´ì–‘í™˜ê²½
   - ìˆ˜ì •ì¼: 2025-09-02
   - URL: https://www.data.go.kr/data/15002978/fileData.do

2. **í•´ì–‘í™˜ê²½ê³µë‹¨_êµ­ê°€í•´ì–‘ìƒíƒœê³„ì¢…í•©ì¡°ì‚¬ ì •ë³´**
   - ì œê³µê¸°ê´€: í•´ì–‘í™˜ê²½ê³µë‹¨
   - ë¶„ë¥˜: í™˜ê²½ê¸°ìƒ - í•´ì–‘í™˜ê²½
   - ìˆ˜ì •ì¼: 2025-09-02
   - URL: https://www.data.go.kr/data/15012624/fileData.do"""
    }
]

example_format = ChatPromptTemplate.from_messages([
    ('human', "{input}"),
    ('ai', "{output}")
])

# 3. í“¨ìƒ· í”„ë¡¬í”„íŠ¸ (ì •ì˜ ë³µêµ¬)
few_shot_prompt = FewShotChatMessagePromptTemplate(
    examples=examples,
    example_prompt=example_format
)

# 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
final_prompt = ChatPromptTemplate.from_messages([
    ('system', '''ë‹¹ì‹ ì€ "Dataset Explorer Agent"ì…ë‹ˆë‹¤. ê³µê³µë°ì´í„° í¬í„¸(data.go.kr)ì˜ ë°ì´í„°ì…‹ì„ ì¶”ì²œí•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**:
1. ì‚¬ìš©ìê°€ ë°ì´í„°ì…‹ì„ ìš”ì²­í•˜ë©´ **ë°˜ë“œì‹œ MCP ë„êµ¬(search_dataset)ë¥¼ ì‚¬ìš©**í•˜ì—¬ ê²€ìƒ‰í•˜ì„¸ìš”.
2. ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê·¸ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

**[ë§¤ìš° ì¤‘ìš”] ì¶œì²˜ í‘œê¸° ì›ì¹™**:
ëª¨ë“  ë‹µë³€ì˜ **ë§¨ ë§ˆì§€ë§‰ ì¤„**ì— ë°˜ë“œì‹œ ì•„ë˜ íƒœê·¸ ì¤‘ í•˜ë‚˜ë¥¼ ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì™¸ëŠ” ì—†ìŠµë‹ˆë‹¤.

1. **MCP ë„êµ¬(search_dataset)ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¸ìš©í•œ ê²½ìš°**:
   > **[ğŸ” ì¶œì²˜: ë‚´ë¶€ ë°ì´í„°ë² ì´ìŠ¤ (FAISS)]**

2. **ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë‚´ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•œ ê²½ìš° (ì¸ì‚¬, ì¼ë°˜ ëŒ€í™” ë“±)**:
   > **[ğŸ¤– ì¶œì²˜: AI ì¼ë°˜ ì§€ì‹]**

**ì‘ë‹µ í˜•ì‹** (ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜):
1. **ë°ì´í„°ì…‹ëª…**
   - ì œê³µê¸°ê´€: XXX
   - ë¶„ë¥˜: XXX
   - ìˆ˜ì •ì¼: YYYY-MM-DD
   - URL: https://www.data.go.kr/...
'''),
    few_shot_prompt,
    MessagesPlaceholder(variable_name="messages")
])
# 5. ìƒíƒœ ì •ì˜
class AgentState(TypedDict, total=False):
    messages: List[BaseMessage]

# 6. ë…¸ë“œ ì •ì˜

# 6-0. ë¼ìš°íŒ… ë…¸ë“œ
def initial_routing_node(state: AgentState):
    messages = state["messages"]
    user_query = ""
    
    # [ìˆ˜ì •] ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤‘ 'ê°€ì¥ ìµœê·¼' ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì•¼ í•¨
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_query = msg.content.lower()
            break
            
    dataset_keywords = ["ì°¾ì•„", "ì¶”ì²œ", "ê²€ìƒ‰", "ë°ì´í„°", "ìë£Œ", "ëª©ë¡", "ì•Œë ¤ì¤˜"]
    
    print(f"\nğŸ§  [AI ì‚¬ê³  (Routing)] ì‚¬ìš©ì ì…ë ¥ ë¶„ì„ ì¤‘: '{user_query}'", flush=True)
    
    if any(keyword in user_query for keyword in dataset_keywords):
        print("   -> ğŸ’¡ ê²°ì •: 'ë°ì´í„° ìš”ì²­' í‚¤ì›Œë“œ ë°œê²¬! -> [ë„êµ¬(Tools)] ì‚¬ìš©", flush=True)
        return {"messages": messages, "_route": "tools"}
    
    print("   -> ğŸ’­ ê²°ì •: íŠ¹ë³„í•œ í‚¤ì›Œë“œ ì—†ìŒ -> [ì¼ë°˜ ëŒ€í™”(Thinking)] ì§„í–‰", flush=True)
    return {"messages": messages, "_route": "thinking"}

# 6-1. Thinking Node (ë‹¨ìˆœ ëŒ€í™”)
def thinking_node(state: AgentState):
    messages = state["messages"]
    print(f"ğŸ¤– [AI ìƒì„±] ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì‹œì‘...", flush=True) # í„°ë¯¸ë„ ë¡œê·¸ ì¶”ê°€
    
    # [ìˆ˜ì •] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
    chain = final_prompt | llm
    res = chain.invoke({"messages": messages})
    return {"messages": [res]}

# 6-2. MCP Tool Node
async def tool_node(state: AgentState):
    messages = state["messages"]
    user_query = None
    
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_query = msg.content
            break
    
    tool_output = "ê²€ìƒ‰í•  ì¿¼ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    if user_query:
        print(f"ğŸ› ï¸ [MCP Tool] ë„êµ¬ 'search_dataset' í˜¸ì¶œ ì¤€ë¹„ ì™„ë£Œ", flush=True)
        print(f"   -> ğŸ“¥ ì…ë ¥ íŒŒë¼ë¯¸í„°: query='{user_query}', k=5", flush=True)
        try:
            # [í•µì‹¬] MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ë„êµ¬ í˜¸ì¶œ (ë¹„ë™ê¸°)
            tool_output = await call_mcp_tool("search_dataset", {"query": user_query, "k": 5})
            print(f"   -> âœ… ë„êµ¬ ì‹¤í–‰ ì™„ë£Œ! ê²°ê³¼ ê¸¸ì´: {len(tool_output)}ì", flush=True)
        except Exception as e:
            tool_output = f"MCP ë„êµ¬ í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            print(f"   -> âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}", flush=True)

    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ì— í¬í•¨
    return {"messages": [
        HumanMessage(content=f"""ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

[MCP ê³µê³µë°ì´í„° ê²€ìƒ‰ê²°ê³¼]:
{tool_output}

ìœ„ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë°ì´í„°ì…‹ ì •ë³´ë¥¼ ì •ë¦¬í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.""")
    ]}

# 6-3. Final Answer Node
def final_answer_node(state: AgentState):
    final_msg = state["messages"]
    
    # [ìˆ˜ì •] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì ìš©
    chain = final_prompt | llm
    res = chain.invoke({"messages": final_msg})
    return {"messages": [res]}

# 7. ê·¸ë˜í”„ ì—°ê²°
workflow = StateGraph(AgentState)
workflow.add_node("routing", initial_routing_node)
workflow.add_node("thinking", thinking_node)
workflow.add_node("tools", tool_node)
workflow.add_node("final_answer", final_answer_node)

workflow.set_entry_point("routing")

def route_decision(state: AgentState):
    return state.get("_route", "thinking")

workflow.add_conditional_edges("routing", route_decision, {"thinking": "thinking", "tools": "tools"})
workflow.add_edge("thinking", END) # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë°”ë¡œ ì¢…ë£Œ
workflow.add_edge("tools", "final_answer")
workflow.add_edge("final_answer", END)

graph_object = workflow.compile()
